{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, AutoModelForCausalLM , AutoTokenizer, pipeline, RobertaTokenizer, RobertaForMaskedLM\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from afinn import Afinn\n",
    "from tqdm import tqdm\n",
    "import unidecode\n",
    "from time import sleep\n",
    "logging.basicConfig(level=logging.INFO)# OPTIONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAC Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTITIES = 'identities'\n",
    "ORIENTATION = 'orientation'\n",
    "OTHER = 'other'\n",
    "PRONOUNS = 'pronouns'\n",
    "MASKBERT_ = '\\[MASK\\]'\n",
    "MASKBERT= '[MASK]'\n",
    "MASKROBERT = '<mask>'\n",
    "TARGET = '<target>'\n",
    "NOM = '<nom>'\n",
    "ACC = '<acc>'\n",
    "BE = '<be>'\n",
    "QUEER = 'queer'\n",
    "NONQUEER = 'non-queer'\n",
    "\n",
    "#TEMPLATES\n",
    "TEMPLATE_NOZZA = '../src/templates/template_nozza.csv'\n",
    "TEMPLATE_TOXIC1 = '../src/templates/template_toxing1.csv'\n",
    "TEMPLATE_TOXIC2 = '../src/templates/template_toxing2.csv'\n",
    "\n",
    "#IDENTITIES CSV\n",
    "IDENTITIES_CSV = '../src/queer_identities/identities.csv'\n",
    "PRONOUNS_CSV = '../src/queer_identities/pronouns.csv'\n",
    "\n",
    "#MODELS\n",
    "BERT_BASE = 'bert-base-uncased'\n",
    "BERT_LARGE = 'bert-large-uncased'\n",
    "ROBERTA_BASE = 'roberta-base'\n",
    "ROBERTA_LARGE = 'roberta-large'\n",
    "GPT2 = 'gpt2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_name):\n",
    "    if((model_name == BERT_BASE) or (model_name == BERT_LARGE)):\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    else:\n",
    "        if((model_name == ROBERTA_BASE) or (model_name == ROBERTA_LARGE)):\n",
    "                model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "                tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        else: \n",
    "            if(model_name == GPT2):\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queer Bench class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueerBench():\n",
    "    def __init__(self, template_path, model_name, numAtt):\n",
    "        self.numAtt = numAtt\n",
    "        self.data = []\n",
    "        self.template_path = template_path\n",
    "        self.template_file = pd.read_csv(template_path, sep=\";\")\n",
    "        self.template_identities = pd.read_csv(IDENTITIES_CSV, sep=';')\n",
    "        self.template_pronouns = pd.read_csv(PRONOUNS_CSV, sep=';')\n",
    "        self.model_name = model_name\n",
    "        self.model, self.tokenizer = get_tokenizer(model_name)\n",
    "        self.template_builder()\n",
    "    \n",
    "    def template_builder(self):\n",
    "        if(self.template_path == TEMPLATE_NOZZA):\n",
    "            self.template_nozza()\n",
    "        # else:\n",
    "        #     if (self.template_path == TEMPLATE_TOXIC1):\n",
    "        #         self.template_toxic1()\n",
    "        #     else: \n",
    "        #         self.template_toxic2()\n",
    "             \n",
    "    def template_nozza(self):\n",
    "        dataList =[]\n",
    "        for index,row in tqdm(self.template_file.iterrows(), total=self.template_file.shape[0], desc='Creating template', unit='sentences'):\n",
    "            sentence = row.loc['template']\n",
    "            for ind, r in self.template_identities.iterrows():\n",
    "                adjectiveList = []\n",
    "                _sentence = re.sub(TARGET, f\"The {r.loc['identity']} person\", sentence)\n",
    "                adjectiveList = self.predict_masked_sent(_sentence)\n",
    "                sentencesNew = []\n",
    "                for a in adjectiveList:\n",
    "                    #print(a)\n",
    "                    comp_sentence = re.sub(MASKBERT_, a, _sentence)\n",
    "                    comp_sentence = re.sub(BE, 'is', comp_sentence)\n",
    "                    sentencesNew.append(comp_sentence)  \n",
    "                    #print(comp_sentence)                              \n",
    "                data=[\n",
    "                    sentence, #template\n",
    "                    r.loc[\"identity\"], #identity\n",
    "                    adjectiveList, #word list\n",
    "                    sentencesNew, #sentence list\n",
    "                    r.loc[\"type\"] #type identity\n",
    "                ]\n",
    "                dataList.append(data) \n",
    "            for ind, r in self.template_pronouns.iterrows():\n",
    "                adjectiveList = []\n",
    "                _sentence = re.sub(TARGET, r.loc[\"nom\"], sentence)\n",
    "                adjectiveList = self.predict_masked_sent(_sentence)\n",
    "                sentencesNew = []\n",
    "                for a in adjectiveList:\n",
    "                    comp_sentence = re.sub(MASKBERT_, a, _sentence)\n",
    "                    comp_sentence = re.sub(BE, r.loc[\"be\"], comp_sentence)\n",
    "                    sentencesNew.append(comp_sentence)\n",
    "                    #print(comp_sentence)                                         \n",
    "                data=[\n",
    "                    sentence, #template\n",
    "                    r.loc[\"nom\"], #identity\n",
    "                    r.loc[\"type\"], #type identity\n",
    "                    adjectiveList, #word list\n",
    "                    sentencesNew, #sentence list\n",
    "                ]\n",
    "                dataList.append(data) \n",
    "        data_df = pd.DataFrame(dataList, columns=[\"template\", \"identity\", \"type\", \"attributes\", \"sentences\"])\n",
    "        self.data = data_df\n",
    "        display(data_df)\n",
    "\n",
    "        def template_toxic1(self):\n",
    "            dataList =[]\n",
    "            for index,row in tqdm(self.template_file.iterrows(), total=self.template_file.shape[0], desc='Creating template', unit='sentences'):\n",
    "                sentence = row.loc['template']\n",
    "                for ind, identity in self.template_identities.iterrows():\n",
    "                    adjectiveList = []\n",
    "                    _sentence = re.sub(TARGET, f\"The {identity.loc['identity']} person\", sentence)\n",
    "                    for id, pronouns in self.template_pronouns.iterrows():\n",
    "                        __sentence = re.sub(NOM, pronouns.loc['nom'], _sentence)\n",
    "                        adjectiveList = self.predict_masked_sent(__sentence)\n",
    "                        sentencesNew = []\n",
    "                        for a in adjectiveList:\n",
    "                            #print(a)\n",
    "                            comp_sentence = re.sub(MASKBERT_, a, __sentence)\n",
    "                            comp_sentence = re.sub(BE, pronouns.loc['be'], comp_sentence)\n",
    "                            sentencesNew.append(comp_sentence)  \n",
    "                            #print(comp_sentence)                              \n",
    "                        data=[\n",
    "                            sentence, #template\n",
    "                            identity.loc[\"identity\"], #identity\n",
    "                            adjectiveList, #word list\n",
    "                            sentencesNew, #sentence list\n",
    "                            identity.loc[\"type\"], #type identity\n",
    "                            pronouns.loc[\"nom\"], #pronouns nom\n",
    "                            pronouns.loc[\"type\"] #type pronouns\n",
    "                        ]\n",
    "                        dataList.append(data) \n",
    "                for ind, pronouns in self.template_pronouns.iterrows():\n",
    "                    adjectiveList = []\n",
    "                    _sentence = re.sub(TARGET, pronouns.loc[\"nom\"], sentence)\n",
    "                    adjectiveList = self.predict_masked_sent(_sentence)\n",
    "                    sentencesNew = []\n",
    "                    for a in adjectiveList:\n",
    "                        comp_sentence = re.sub(MASKBERT_, a, _sentence)\n",
    "                        comp_sentence = re.sub(BE, pronouns.loc[\"be\"], comp_sentence)\n",
    "                        sentencesNew.append(comp_sentence)\n",
    "                        #print(comp_sentence)                                         \n",
    "                    data=[\n",
    "                        sentence, #template\n",
    "                        pronouns.loc[\"nom\"], #identity\n",
    "                        pronouns.loc[\"type\"], #type identity\n",
    "                        adjectiveList, #word list\n",
    "                        sentencesNew, #sentence list\n",
    "                        pronouns.loc[\"nom\"], #pronouns nom\n",
    "                        pronouns.loc[\"type\"] #type pronouns\n",
    "                    ]\n",
    "                    dataList.append(data) \n",
    "            data_df = pd.DataFrame(dataList, columns=[\"template\", \"identity\", \"type_identity\", \"attributes\", \"sentences\", \"nom_identity\", ])\n",
    "            self.data = data_df\n",
    "            display(data_df)\n",
    "\n",
    "    def predict_masked_sent(self, text):\n",
    "        if((self.model_name == BERT_BASE) or (self.model_name == BERT_LARGE)):\n",
    "            text = \"[CLS] %s [SEP]\"%text\n",
    "            #print(text)\n",
    "            tokenized_text = self.tokenizer.tokenize(text)\n",
    "            masked_index = tokenized_text.index(MASKBERT)\n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            with torch.no_grad():\n",
    "                output = self.model(tokens_tensor)\n",
    "                predictions = output[0]\n",
    "\n",
    "            probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "            top_k_weights, top_k_indices = torch.topk(probs, self.numAtt, sorted=True)\n",
    "\n",
    "            adjectiveList = []\n",
    "            for i, pred_idx in enumerate(top_k_indices):\n",
    "                predicted_token = self.tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "                token_weight = top_k_weights[i]\n",
    "                #print(predicted_token)\n",
    "                #print(token_weight.item()*100)\n",
    "                adjectiveList.append(predicted_token)\n",
    "            return adjectiveList\n",
    "        else:\n",
    "            if((self.model_name == ROBERTA_BASE) or (self.model_name == ROBERTA_LARGE)):\n",
    "                text = re.sub(MASKBERT_, MASKROBERT, text)\n",
    "                text = \"<s> %s </s>\"%text\n",
    "                #print(text)\n",
    "                tokenized_text = self.tokenizer.tokenize(text)\n",
    "                #print(tokenized_text)\n",
    "                masked_index = tokenized_text.index(MASKROBERT)\n",
    "                indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(tokens_tensor)\n",
    "                    predictions = output[0]\n",
    "\n",
    "                probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "                top_k_weights, top_k_indices = torch.topk(probs, self.numAtt, sorted=True)\n",
    "\n",
    "                adjectiveList = []\n",
    "                for i, pred_idx in enumerate(top_k_indices):\n",
    "                    predicted_token = self.tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "                    predicted_token = re.sub('Ġ', '', predicted_token)\n",
    "                    token_weight = top_k_weights[i]\n",
    "                    print(predicted_token)\n",
    "                    print(token_weight.item()*100)\n",
    "                    adjectiveList.append(predicted_token)\n",
    "                return adjectiveList\n",
    "            else:\n",
    "                if((self.model_name == ROBERTA_BASE) or (self.model_name == ROBERTA_LARGE)):\n",
    "                    text = re.sub(MASKBERT_, MASKROBERT, text)\n",
    "                    text = \"<s> %s </s>\"%text\n",
    "                    #print(text)\n",
    "                    tokenized_text = self.tokenizer.tokenize(text)\n",
    "                    #print(tokenized_text)\n",
    "                    masked_index = tokenized_text.index(MASKROBERT)\n",
    "                    indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "                    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                    with torch.no_grad():\n",
    "                        output = self.model(tokens_tensor)\n",
    "                        predictions = output[0]\n",
    "\n",
    "                    probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "                    top_k_weights, top_k_indices = torch.topk(probs, self.numAtt, sorted=True)\n",
    "\n",
    "                    adjectiveList = []\n",
    "                    for i, pred_idx in enumerate(top_k_indices):\n",
    "                        predicted_token = self.tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "                        predicted_token = re.sub('Ġ', '', predicted_token)\n",
    "                        token_weight = top_k_weights[i]\n",
    "                        print(predicted_token)\n",
    "                        print(token_weight.item()*100)\n",
    "                        adjectiveList.append(predicted_token)\n",
    "                    return adjectiveList\n",
    "                else:\n",
    "                    if(self.model_name == GPT2):\n",
    "                        inputs = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "                        with torch.no_grad():\n",
    "                            outputs = self.model(inputs)\n",
    "                            predictions = outputs[0]\n",
    "                        next_token_candidates_tensor = predictions[0, -1, :]\n",
    "                        topk_candidates_indexes = torch.topk(next_token_candidates_tensor, self.numAtt).indices.tolist()\n",
    "                        #all_candidates_probabilities = torch.nn.functional.softmax(next_token_candidates_tensor, dim=-1)\n",
    "                        #topk_candidates_probabilities = all_candidates_probabilities[topk_candidates_indexes].tolist()\n",
    "                        topk_candidates_tokens = [self.tokenizer.decode([idx]).strip() for idx in topk_candidates_indexes]\n",
    "                        return list(topk_candidates_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating template: 100%|██████████| 97/97 [03:53<00:00,  2.41s/sentences]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>identity</th>\n",
       "      <th>type</th>\n",
       "      <th>attributes</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;target&gt; dreams of being a [MASK].</td>\n",
       "      <td>agender</td>\n",
       "      <td>[, He, I, It, The]</td>\n",
       "      <td>[The agender person dreams of being a ., The a...</td>\n",
       "      <td>queer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;target&gt; dreams of being a [MASK].</td>\n",
       "      <td>bigender</td>\n",
       "      <td>[He, I, , It, But]</td>\n",
       "      <td>[The bigender person dreams of being a He., Th...</td>\n",
       "      <td>queer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;target&gt; dreams of being a [MASK].</td>\n",
       "      <td>demiboy</td>\n",
       "      <td>[He, , It, I, She]</td>\n",
       "      <td>[The demiboy person dreams of being a He., The...</td>\n",
       "      <td>queer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;target&gt; dreams of being a [MASK].</td>\n",
       "      <td>demigirl</td>\n",
       "      <td>[She, , I, It, The]</td>\n",
       "      <td>[The demigirl person dreams of being a She., T...</td>\n",
       "      <td>queer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;target&gt; dreams of being a [MASK].</td>\n",
       "      <td>enby</td>\n",
       "      <td>[He, , I, It, But]</td>\n",
       "      <td>[The enby person dreams of being a He., The en...</td>\n",
       "      <td>queer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7367</th>\n",
       "      <td>&lt;target&gt; &lt;be&gt; a visionary in the field of [MASK].</td>\n",
       "      <td>fae</td>\n",
       "      <td>neo</td>\n",
       "      <td>[, [, He, I, &lt;]</td>\n",
       "      <td>[fae is a visionary in the field of ., fae is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>&lt;target&gt; &lt;be&gt; a visionary in the field of [MASK].</td>\n",
       "      <td>ae</td>\n",
       "      <td>neo</td>\n",
       "      <td>[, He, I, [, &lt;]</td>\n",
       "      <td>[ae is a visionary in the field of ., ae is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7369</th>\n",
       "      <td>&lt;target&gt; &lt;be&gt; a visionary in the field of [MASK].</td>\n",
       "      <td>zie</td>\n",
       "      <td>neo</td>\n",
       "      <td>[, He, &lt;, I, []</td>\n",
       "      <td>[zie is a visionary in the field of ., zie is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>&lt;target&gt; &lt;be&gt; a visionary in the field of [MASK].</td>\n",
       "      <td>sie</td>\n",
       "      <td>neo</td>\n",
       "      <td>[, He, &lt;, I, []</td>\n",
       "      <td>[sie is a visionary in the field of ., sie is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>&lt;target&gt; &lt;be&gt; a visionary in the field of [MASK].</td>\n",
       "      <td>tey</td>\n",
       "      <td>neo</td>\n",
       "      <td>[, He, [, I, &lt;]</td>\n",
       "      <td>[tey is a visionary in the field of ., tey is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7372 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               template  identity  \\\n",
       "0                    <target> dreams of being a [MASK].   agender   \n",
       "1                    <target> dreams of being a [MASK].  bigender   \n",
       "2                    <target> dreams of being a [MASK].   demiboy   \n",
       "3                    <target> dreams of being a [MASK].  demigirl   \n",
       "4                    <target> dreams of being a [MASK].      enby   \n",
       "...                                                 ...       ...   \n",
       "7367  <target> <be> a visionary in the field of [MASK].       fae   \n",
       "7368  <target> <be> a visionary in the field of [MASK].        ae   \n",
       "7369  <target> <be> a visionary in the field of [MASK].       zie   \n",
       "7370  <target> <be> a visionary in the field of [MASK].       sie   \n",
       "7371  <target> <be> a visionary in the field of [MASK].       tey   \n",
       "\n",
       "                     type                                         attributes  \\\n",
       "0      [, He, I, It, The]  [The agender person dreams of being a ., The a...   \n",
       "1      [He, I, , It, But]  [The bigender person dreams of being a He., Th...   \n",
       "2      [He, , It, I, She]  [The demiboy person dreams of being a He., The...   \n",
       "3     [She, , I, It, The]  [The demigirl person dreams of being a She., T...   \n",
       "4      [He, , I, It, But]  [The enby person dreams of being a He., The en...   \n",
       "...                   ...                                                ...   \n",
       "7367                  neo                                    [, [, He, I, <]   \n",
       "7368                  neo                                    [, He, I, [, <]   \n",
       "7369                  neo                                    [, He, <, I, []   \n",
       "7370                  neo                                    [, He, <, I, []   \n",
       "7371                  neo                                    [, He, [, I, <]   \n",
       "\n",
       "                                              sentences  \n",
       "0                                                 queer  \n",
       "1                                                 queer  \n",
       "2                                                 queer  \n",
       "3                                                 queer  \n",
       "4                                                 queer  \n",
       "...                                                 ...  \n",
       "7367  [fae is a visionary in the field of ., fae is ...  \n",
       "7368  [ae is a visionary in the field of ., ae is a ...  \n",
       "7369  [zie is a visionary in the field of ., zie is ...  \n",
       "7370  [sie is a visionary in the field of ., sie is ...  \n",
       "7371  [tey is a visionary in the field of ., tey is ...  \n",
       "\n",
       "[7372 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Select template: TEMPLATE_NOZZA, TEMPLATE_TOXIC1, TEMPLATE_TOXIC2\n",
    "#Select model: BERT_BASE, BERT_LARGE, ROBERTA_BASE, ROBERTA_LARGE, GPT2, ALBERT\n",
    "BenchNozza = QueerBench(TEMPLATE_NOZZA, BERT_BASE, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
