{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, AutoModelForCausalLM , AutoTokenizer, pipeline, RobertaTokenizer, RobertaForMaskedLM, AlbertTokenizer, AlbertModel, AlbertForMaskedLM\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from afinn import Afinn\n",
    "from tqdm import tqdm\n",
    "import unidecode\n",
    "from time import sleep\n",
    "logging.basicConfig(level=logging.INFO)# OPTIONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAC Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTITIES = 'identities'\n",
    "ORIENTATION = 'orientation'\n",
    "OTHER = 'other'\n",
    "PRONOUNS = 'pronouns'\n",
    "MASKBERT_ = '\\[MASK\\]'\n",
    "MASKBERT= '[MASK]'\n",
    "MASKROBERT = '<mask>'\n",
    "TARGET = '<target>'\n",
    "NOM = '<nom>'\n",
    "POSDEP = '<pos_dep>'\n",
    "ACC = '<acc>'\n",
    "BE = '<be>'\n",
    "QUEER = 'queer'\n",
    "NONQUEER = 'non-queer'\n",
    "\n",
    "#TEMPLATES\n",
    "TEMPLATE_NOZZA = '../src/templates/template_nozza.csv'\n",
    "TEMPLATE_NOZZA_COMPLETE = '../src/templates/template_nozza_complete.csv'\n",
    "TEMPLATE_TOXIC2 = '../src/templates/template_toxic2.csv'\n",
    "TEMPLATE_TOXIC2_COMPLETE = '../src/templates/template_toxic2_complete.csv'\n",
    "TEMPLATE_TOXIC1 = '../src/templates/template_toxic1.csv'\n",
    "TEMPLATE_TOXIC1_COMPLETE = '../src/templates/template_toxic1_complete.csv'\n",
    "TEMPLATE_TOXIC1_CHUNK = '../src/templates/toxic1/template_toxic1'\n",
    "TEMPLATE_TOXIC2_CHUNK = '../src/templates/toxic2/template_toxic2'\n",
    "PREDICTION_PATH = \"../src/prediction/\"\n",
    "\n",
    "#IDENTITIES CSV\n",
    "IDENTITIES_CSV = '../src/queer_identities/identities.csv'\n",
    "PRONOUNS_CSV = '../src/queer_identities/pronouns.csv'\n",
    "\n",
    "#MODELS\n",
    "BERT_BASE = 'bert-base-uncased'\n",
    "BERT_LARGE = 'bert-large-uncased'\n",
    "ROBERTA_BASE = 'roberta-base'\n",
    "ROBERTA_LARGE = 'roberta-large'\n",
    "GPT2 = 'gpt2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Prediction class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplatePrediction:\n",
    "    def __init__(self, model_name, template_path, numAtt):\n",
    "        self.numAtt = numAtt\n",
    "        self.template_path = template_path\n",
    "        self.model_name = model_name\n",
    "        self.model, self.tokenizer = self.get_tokenizer()\n",
    "        self.template_prediction()\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        if((self.model_name == BERT_BASE) or (self.model_name == BERT_LARGE)):\n",
    "            model = BertForMaskedLM.from_pretrained(self.model_name)\n",
    "            tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        else:\n",
    "            if((self.model_name == ROBERTA_BASE) or (self.model_name == ROBERTA_LARGE)):\n",
    "                    model = RobertaForMaskedLM.from_pretrained(self.model_name)\n",
    "                    tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n",
    "            else: \n",
    "                if(self.model_name == GPT2):\n",
    "                    model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def template_prediction(self):\n",
    "        if self.template_path == TEMPLATE_NOZZA_COMPLETE:\n",
    "            self.template_nozza()\n",
    "        else:\n",
    "            if self.template_path == TEMPLATE_TOXIC1_COMPLETE:\n",
    "                self.template_toxic1()\n",
    "                \n",
    "\n",
    "    def template_nozza(self):\n",
    "        prediction = []\n",
    "        template = pd.read_csv(self.template_path, sep=\";\")\n",
    "        for index,row in tqdm(template.iterrows(), total=template.shape[0], desc='Predicting mask', unit='sentences'):\n",
    "            sentence = row.loc['new_template']\n",
    "            model_prediction = self.model_prediction(sentence)\n",
    "            prediction.append(model_prediction)\n",
    "        template.loc[:,'prediction'] = prediction\n",
    "        display(template)\n",
    "        template.to_csv(PREDICTION_PATH+self.model_name+\"/template_nozza.csv\", sep=';')\n",
    "\n",
    "    def template_toxic1(self):\n",
    "        prediction = []\n",
    "        for i in tqdm(range(20), total=20, desc='Chunk number', unit='chunk'):\n",
    "            template = pd.read_csv(TEMPLATE_TOXIC1_CHUNK+'_chunk{}.csv'.format(i)) #, sep=';', dtype={'new_template':'category', 'identity': 'category', 'type_identity': 'category','pronoun': 'category', 'pronouns_type': 'category'})\n",
    "            for index,row in tqdm(template.iterrows(), total=template.shape[0], desc='Predicting mask', unit='sentences'):\n",
    "                sentence = row.loc['new_template']\n",
    "                print(sentence)\n",
    "                model_prediction = self.model_prediction(sentence)\n",
    "                prediction.append(model_prediction)\n",
    "            self.template.loc[:,'prediction'] = prediction\n",
    "            display(self.template)\n",
    "            #self.template.to_csv(TEMPLATE_TOXIC1_CHUNK+'_chunk{}.csv', sep=';')\n",
    "\n",
    "    def model_prediction(self, text):\n",
    "        if((self.model_name == BERT_BASE) or (self.model_name == BERT_LARGE)):\n",
    "            return self.bert_prediction(text)\n",
    "        else:\n",
    "            if((self.model_name == ROBERTA_BASE) or (self.model_name == ROBERTA_LARGE)):\n",
    "                return self.roberta_prediction(text)\n",
    "            else:\n",
    "                if(self.model_name == GPT2):\n",
    "                        return self.gpt2_prediction(text)\n",
    "                \n",
    "    def bert_prediction(self, text):\n",
    "        text = \"[CLS] %s [SEP]\"%text\n",
    "        #print(text)\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        masked_index = tokenized_text.index(MASKBERT)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        with torch.no_grad():\n",
    "            output = self.model(tokens_tensor)\n",
    "            predictions = output[0]\n",
    "\n",
    "        probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "        top_k_weights, top_k_indices = torch.topk(probs, self.numAtt, sorted=True)\n",
    "\n",
    "        adjectiveList = []\n",
    "        for i, pred_idx in enumerate(top_k_indices):\n",
    "            predicted_token = self.tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "            token_weight = top_k_weights[i]\n",
    "            #print(predicted_token)\n",
    "            #print(token_weight.item()*100)\n",
    "            adjectiveList.append(predicted_token)\n",
    "        return adjectiveList\n",
    "    \n",
    "    def roberta_prediction(self, text):\n",
    "        text = re.sub(MASKBERT_, MASKROBERT, text)\n",
    "        text = \"<s> %s </s>\"%text\n",
    "        #print(text)\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        #print(tokenized_text)\n",
    "        masked_index = tokenized_text.index(MASKROBERT)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        with torch.no_grad():\n",
    "            output = self.model(tokens_tensor)\n",
    "            predictions = output[0]\n",
    "\n",
    "        probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "        top_k_weights, top_k_indices = torch.topk(probs, self.numAtt, sorted=True)\n",
    "\n",
    "        adjectiveList = []\n",
    "        for i, pred_idx in enumerate(top_k_indices):\n",
    "            predicted_token = self.tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "            predicted_token = re.sub('Ä ', '', predicted_token)\n",
    "            token_weight = top_k_weights[i]\n",
    "            print(predicted_token)\n",
    "            print(token_weight.item()*100)\n",
    "            adjectiveList.append(predicted_token)\n",
    "        return adjectiveList\n",
    "        \n",
    "    def gpt2_prediction(self, text):\n",
    "        inputs = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            predictions = outputs[0]\n",
    "        next_token_candidates_tensor = predictions[0, -1, :]\n",
    "        topk_candidates_indexes = torch.topk(next_token_candidates_tensor, self.numAtt).indices.tolist()\n",
    "        #all_candidates_probabilities = torch.nn.functional.softmax(next_token_candidates_tensor, dim=-1)\n",
    "        #topk_candidates_probabilities = all_candidates_probabilities[topk_candidates_indexes].tolist()\n",
    "        topk_candidates_tokens = [self.tokenizer.decode([idx]).strip() for idx in topk_candidates_indexes]\n",
    "        return list(topk_candidates_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TemplatePrediction(BERT_BASE, TEMPLATE_NOZZA_COMPLETE, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
